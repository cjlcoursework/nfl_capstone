{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Summary\n",
    "\n",
    "## inputs\n",
    "This notebook has three inputs:\n",
    "1. A reviewed and slightly cleaned version of NFL **nflplaybyplay2009to2016** having many rows per game - each row is a 'play' in the games\n",
    "2. A **dimensions** dataset from our initial review notebook - this categorized each column by how it should be treated\n",
    "3. An NFL **nfl_teams_scraped** dataset that matches team names to the abbreviation used in the gameplay data  (e.g. Green Bay Packers == GB)\n",
    "    3.1 I've since found another list on kaggle - but this one works well enough\n",
    "\n",
    "The gameplay data has many nulls(), but they make sense once we recognize that not every field is applicable for every type of play.\n",
    ">> for example,\n",
    "> If a row represents a passing play, then the rushing data does not make sense, so it's all null\n",
    ">\n",
    "\n",
    "\n",
    "## goal\n",
    "To create datasets that might not yet be completely prepared for ML, but can be queried for many uses, including ML\n",
    "\n",
    "## cleanup\n",
    "1. Separate the data into core **facts** - these are columns that apply to every play, and should never be null\n",
    "2. Create a separate dataset for all the **dimensions** columns that are only good for specific kinds of plays\n",
    "3. Add in facts that are inferred by the sparse dimensions columns, but don't explicitly exist as facts:\n",
    ">> for example:\n",
    ">     If there was a defensive two point conversion - the def_two_point will be non-null\n",
    ">           but it is null for every other case\n",
    ">           see we create a def_two_point_key that is always 1 or 0 in the fact table\n",
    ">           and we move def_two_point the dimensions\n",
    "> There are cases where we could just fill the def_two_point with 'Not Applicable' when it's null,\n",
    "> but that's not ging to solve every issue\n",
    ">\n",
    "4. Identify boolean keys that are important pivots in the facts table:\n",
    "    (a) whether a pass was attempted\n",
    "    (b) whether a RUSH was attempted\n",
    "    (c) whether there was a penalty on the play\n",
    "    (d) an offensive or defensive two point conversion\n",
    "    (e) whether there was a sack\n",
    "    (f) whether a pass was attempted\n",
    " ... and more...\n",
    "\n",
    "## outputs\n",
    "1. A cleaned NFL `gameplay` dataset - having many rows per game - each row is a 'play' in the games\n",
    "2. A column-level metrics dataset that holds some key metrics from describe(), dtypes, etc. and also some configurations\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "column definitions - thanks to https://github.com/maksimhorowitz/nflscrapR/blob/master/R/scrape_play_by_play.R\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## metrics\n",
    "Looking at the metrics data - the final output gameplay dataset should be almost complete, with a small enough set of nulls that can be reviewd manually (52)\n",
    "The completeness column is just the amount of non-null records divided by the total record count"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<span><img src=\"metrics_clean_01.png\" width=\"2500\"></span>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TODO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "outputs": [],
   "source": [
    "# todo - is the play_recorded key really helpful?\n",
    "# todo - remove the inconsistent playtype column or update it\n",
    "# todo - review the No Play conversion - if the playtype is no good, why fix it halfway?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "# comments: <span style=\"color:#20B2AA\">\n",
    "\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding modules /Users/christopherlomeli/Source/courses/datascience/nfl_capstone/src\n"
     ]
    }
   ],
   "source": [
    "module_path = os.path.abspath(os.path.join('../src'))\n",
    "print(\"Adding modules\", module_path)\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "outputs": [],
   "source": [
    "from src.features.wrangling.database_loader import DatabaseLoader\n",
    "from src.features.wrangling.get_metrics import GetMetrics, update_by_lookp\n",
    "import src.data.nfl_utils as nfl"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "outputs": [],
   "source": [
    "AWS_S3_BUCKET = \"cjl-project-data\"\n",
    "AWS_S3_PREFIX = \"nfl_capstone/data/raw\"\n",
    "\n",
    "RAW_DATA_PATH = '../data/raw'\n",
    "INTERIM_DATA_PATH = '../data/interim'\n",
    "USE_CONNECTION = \"DB_FILENAME_URL\"  # DB_FILENAME_URL for csv or DB_CONNECTION_URL for postgres\n",
    "\n",
    "#inputs\n",
    "DATA_FILE = os.path.join(INTERIM_DATA_PATH, \"nflverse.2009.reviewed.parquet\")\n",
    "TEAMS_DATA = os.path.join(RAW_DATA_PATH, \"nfl_teams_scraped.csv\")\n",
    "DIMENSIONS_DATA = os.path.join(RAW_DATA_PATH, \"dimensions.csv\")\n",
    "\n",
    "#outputs\n",
    "GAMEPLAY_FACTS_DF_NAME = os.path.join(INTERIM_DATA_PATH, \"gameplay_facts_cleaned_01.parquet\")\n",
    "GAMEPLAY_DIM_DF_NAME = os.path.join(INTERIM_DATA_PATH, \"gameplay_dimensions_cleaned_01.parquet\")\n",
    "ANALYTICS_DF_NAME = os.path.join(INTERIM_DATA_PATH, \"analytic_events_cleaned_01.parquet\")\n",
    "ADMIN_DF_NAME = os.path.join(INTERIM_DATA_PATH, \"admin_events_cleaned_01.parquet\")\n",
    "READ_ME = os.path.join(INTERIM_DATA_PATH, \"README.02-cjl-clean.txt\")\n",
    "\n",
    "# tables\n",
    "METRICS_INPUT_TABLE_NAME = \"nfl_metrics\"\n",
    "CATEGORY_OUTPUT_TABLE_NAME = \"nfl_cleaned_categories\"\n",
    "METRICS_OUTPUT_TABLE_NAME = \"nfl_cleaned_metrics\"\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## read data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists:  /Users/christopherlomeli/Source/courses/datascience/nfl_capstone/data/raw/dimensions.csv\n",
      "Already exists:  /Users/christopherlomeli/Source/courses/datascience/nfl_capstone/data/raw/nfl_teams_scraped.csv\n"
     ]
    }
   ],
   "source": [
    "nfl.copy_raw_files_to_local(\n",
    "    bucket=AWS_S3_BUCKET,\n",
    "    prefix=AWS_S3_PREFIX,\n",
    "    local_dir=os.path.abspath(RAW_DATA_PATH),\n",
    "    file_names=['nfl_teams_scraped.csv', 'dimensions.csv'])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input data file has shape  (48034, 384)\n"
     ]
    }
   ],
   "source": [
    "data_df = nfl.get_local_parquet(DATA_FILE)\n",
    "assert data_df.shape[0] > 0\n",
    "print(\"input data file has shape \", data_df.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric table has shape  (384, 17)\n"
     ]
    }
   ],
   "source": [
    "db = DatabaseLoader(connection_string_env_url=USE_CONNECTION)\n",
    "metrics_df = db.read_table(METRICS_INPUT_TABLE_NAME)\n",
    "assert metrics_df.shape[0] > 0\n",
    "print(\"metric table has shape \", metrics_df.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conversions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## todo - create a new play id feild in ALL record"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "outputs": [],
   "source": [
    "data_df = nfl.add_unique_play_id(\n",
    "    data_df=data_df,\n",
    "    base_column=\"game_id\",\n",
    "    unique_column_name=\"row_id\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## split analytic, fact and dimension columns into thier own dataframes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape before splitting columns (48034, 385)\n",
      "data shape after splitting columns (48034, 161)\n"
     ]
    }
   ],
   "source": [
    "data_df, dimensions_df, analytics_df = nfl.split_gameplay_columns(\n",
    "    data_df=data_df,\n",
    "    metrics_df=metrics_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "outputs": [
    {
     "data": {
      "text/plain": "Empty DataFrame\nColumns: [column_name, data_type, unique_counts, feature_type, c_dimension, row_count, good_count, missing_count, completeness, quality, mean, std, min, max, median, top, freq]\nIndex: []",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>column_name</th>\n      <th>data_type</th>\n      <th>unique_counts</th>\n      <th>feature_type</th>\n      <th>c_dimension</th>\n      <th>row_count</th>\n      <th>good_count</th>\n      <th>missing_count</th>\n      <th>completeness</th>\n      <th>quality</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>min</th>\n      <th>max</th>\n      <th>median</th>\n      <th>top</th>\n      <th>freq</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df.loc[(metrics_df.missing_count == 0) & (metrics_df.unique_counts == metrics_df.row_count)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 04 separate admin from gameplay rows\n",
    "Several rows are for administrative events such as timeout, END Game, or End Quarter -- which creates a lot of nulls and not-applicable values\n",
    "We probably don't want them, so at least segment them as gameplay = Yes or No, where gameplay=No signifies an administrative event, not a real play"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "separate_admin_events: data shape before splitting rows (48034, 162)\n",
      "separate_admin_events: admin_events shape (4058, 162)\n",
      "separate_admin_events: game_events shape (43976, 162)\n"
     ]
    }
   ],
   "source": [
    "# now data_df does not contain any dimension or analytic columns,\n",
    "# and now we want to segment this into game play events vs. admin events like \"START GAME\"\n",
    "data_df, admin_df = nfl.separate_admin_rows(data_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 09 validate team names against our teams dataframe\n",
    "Several fields are populated with the team abbreviation (e.g. LA Rams == 'LAR')\n",
    "Some of these abbreviations are historical and no longer exist\n",
    "Others are errors -"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "outputs": [
    {
     "data": {
      "text/plain": "['ARI',\n 'ATL',\n 'BAL',\n 'BUF',\n 'CAR',\n 'CHI',\n 'CIN',\n 'CLE',\n 'DAL',\n 'DEN',\n 'DET',\n 'GB',\n 'HOU',\n 'IND',\n 'KC',\n 'MIA',\n 'MIN',\n 'NE',\n 'NO',\n 'NYG',\n 'NYJ',\n 'PHI',\n 'PIT',\n 'SF',\n 'SEA',\n 'TB',\n 'TEN',\n 'WAS',\n 'SD',\n 'LAC',\n 'LV',\n 'OAK',\n 'LAR',\n 'STL',\n 'JAX',\n 'JAC']"
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a control list of team names and abbreviations\n",
    "team_df = pd.read_csv(TEAMS_DATA)\n",
    "teams = list(team_df.Abbreviation)\n",
    "teams\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COLUMN:  away_team\n",
      "['LA']\n",
      "-----------------------------------\n",
      "COLUMN:  home_team\n",
      "['LA']\n",
      "-----------------------------------\n",
      "COLUMN:  posteam_type\n",
      "['away' 'home']\n",
      "-----------------------------------\n",
      "COLUMN:  posteam\n",
      "['LA']\n",
      "-----------------------------------\n",
      "COLUMN:  defteam\n",
      "['LA']\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# create a function to list al team abbreviations that are not in our control list\n",
    "team_columns = ['away_team',\n",
    "                'home_team',\n",
    "                'posteam_type',\n",
    "                'posteam',\n",
    "                'defteam'\n",
    "                ]\n",
    "\n",
    "\n",
    "def validate_teams(data_df, team_columns):\n",
    "    for t in team_columns:\n",
    "        print(\"COLUMN: \", t)\n",
    "        print(data_df.loc[~data_df[t].isin(list(teams)), t].unique())\n",
    "        print(\"-----------------------------------\")\n",
    "\n",
    "\n",
    "validate_teams(data_df, team_columns)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update COLUMN:  away_team\n",
      "Update COLUMN:  home_team\n",
      "Update COLUMN:  posteam_type\n",
      "Update COLUMN:  posteam\n",
      "Update COLUMN:  defteam\n"
     ]
    }
   ],
   "source": [
    "# cleanup the ones we know about\n",
    "for t in team_columns:\n",
    "    print(\"Update COLUMN: \", t)\n",
    "    data_df.loc[data_df[t] == 'LA', t] = 'LAR'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "outputs": [
    {
     "data": {
      "text/plain": "KICK_OFF      2724\nXP_KICK       1270\nPAT2           116\nPENALTY         32\nFIELD_GOAL       1\nFREE_KICK        1\nName: play_type_nfl, dtype: int64"
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "down is null when we have a non-passing or rushing play, such as Kickoff, Field goal, Free kick, etc.\n",
    "\"\"\"\n",
    "data_df.loc[(data_df.down.isna()), 'play_type_nfl'].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "outputs": [
    {
     "data": {
      "text/plain": "8               pass\n9        extra_point\n18               run\n19       extra_point\n61               run\n            ...     \n47995    extra_point\n48007           pass\n48008    extra_point\n48021            run\n48022    extra_point\nName: play_type, Length: 3492, dtype: object"
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "end_yard_line is null when points have been scored\n",
    "\"\"\"\n",
    "data_df.loc[(data_df.end_yard_line.isna()), 'play_type']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "down type float32\n",
      "end_yard_line type object\n"
     ]
    }
   ],
   "source": [
    "print(\"down type\", data_df.down.dtype)\n",
    "print(\"end_yard_line type\", data_df.end_yard_line.dtype)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "outputs": [],
   "source": [
    "data_df.down.fillna(0, inplace=True)\n",
    "data_df.own_kickoff_recovery.fillna(0, inplace=True)\n",
    "data_df.drive_inside20.fillna(0, inplace=True)\n",
    "data_df.drive_ended_with_score.fillna(0, inplace=True)\n",
    "data_df.success.fillna(0, inplace=True)\n",
    "data_df.down.fillna(0, inplace=True)\n",
    "\n",
    "data_df.end_yard_line.fillna('N/A', inplace=True)\n",
    "data_df.drive_end_yard_line.fillna('N/A', inplace=True)\n",
    "data_df.drive_start_yard_line.fillna('N/A', inplace=True)\n",
    "\n",
    "data_df.play_type_nfl.fillna('PENALTY', inplace=True)  # verified by sight that these are penalties\n",
    "data_df.play_type.fillna('penalty', inplace=True)  # verified by sight that these are penalties\n",
    "data_df.qb_dropback.fillna(0, inplace=True)  # verified by sight that these are penalties\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "time_of_day is null when we have a non-passing or rushing play, such as Kickoff, Field goal, Free kick, etc.\n",
    "it's a time object '23:01:59'and we may not need it, even though there are only 365 missing, and it coud be helpful, so lets go ahead and fill it with '99:99:99' so it will fail if we try to convert it to a datetime\n",
    "if we decide to use it them we'd have to lag to fill in with the previous time\n",
    "\n",
    "\"\"\"\n",
    "data_df.loc[(data_df.time_of_day.isna()), 'play_type_nfl'].value_counts()\n",
    "data_df.drop(['time_of_day'], axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "booleans with nulls for penalties and kickoffs -are all binary data with nulls for kickoffs and penalties\n",
    "So it can be 0 for these\n",
    "\"\"\"\n",
    "flags = {'tackled_for_loss': 0,\n",
    "         'penalty': 0,\n",
    "         'safety': 0,\n",
    "         'solo_tackle': 0,\n",
    "         'fumble_out_of_bounds': 0,\n",
    "        'success': 0,\n",
    "        'drive_inside20': 0,\n",
    "        'drive_first_downs': 0,\n",
    "        'drive_ended_with_score': 0,\n",
    "        'drive_play_id_ended': 0,\n",
    "        'own_kickoff_recovery': 0,\n",
    "        'drive_play_count': 0,\n",
    "        'ydsnet': 0,\n",
    "        'drive': 0,\n",
    "        'drive_yards_penalized': 0,\n",
    "        'drive_quarter_end': 0,\n",
    "        'drive_play_id_started': 0,\n",
    "        'drive_quarter_start': 0,\n",
    "         'fumble_not_forced': 0,\n",
    "         'fumble_forced': 0,\n",
    "         'kickoff_fair_catch': 0,\n",
    "         'kickoff_downed': 0,\n",
    "         'kickoff_inside_twenty': 0,\n",
    "         'kickoff_in_endzone': 0,\n",
    "         'fumble_lost': 0,\n",
    "         'punt_fair_catch': 0,\n",
    "         'punt_downed': 0,\n",
    "         'punt_out_of_bounds': 0,\n",
    "         'punt_in_endzone': 0,\n",
    "         'punt_inside_twenty': 0,\n",
    "         'kickoff_attempt': 0,\n",
    "         'interception': 0,\n",
    "         'incomplete_pass': 0,\n",
    "         'fourth_down_failed': 0,\n",
    "         'kickoff_out_of_bounds': 0,\n",
    "         'own_kickoff_recovery_td': 0,\n",
    "         'qb_hit': 0,\n",
    "         'rush_attempt': 0,\n",
    "         'defensive_extra_point_conv': 0,\n",
    "         'defensive_extra_point_attempt': 0,\n",
    "         'defensive_two_point_conv': 0,\n",
    "         'defensive_two_point_attempt': 0,\n",
    "         'return_yards': 0,\n",
    "         'tackle_with_assist': 0,\n",
    "         'lateral_recovery': 0,\n",
    "         'lateral_return': 0,\n",
    "         'lateral_rush': 0,\n",
    "         'lateral_reception': 0,\n",
    "         'assist_tackle': 0,\n",
    "         'complete_pass': 0,\n",
    "         'fumble': 0,\n",
    "         'punt_attempt': 0,\n",
    "         'field_goal_attempt': 0,\n",
    "         'two_point_attempt': 0,\n",
    "         'extra_point_attempt': 0,\n",
    "         'return_touchdown': 0,\n",
    "         'rush_touchdown': 0,\n",
    "         'pass_touchdown': 0,\n",
    "         'touchdown': 0,\n",
    "         'sack': 0,\n",
    "         'pass_attempt': 0,\n",
    "         'fourth_down_converted': 0,\n",
    "         'third_down_failed': 0,\n",
    "         'third_down_converted': 0,\n",
    "         'first_down_pass': 0,\n",
    "         'first_down_rush': 0,\n",
    "         'punt_blocked': 0,\n",
    "         'timeout': 0,\n",
    "         'first_down_penalty': 0,\n",
    "         'first_down': 0,\n",
    "         'yards_gained': 0}\n",
    "\n",
    "data_df.fillna(flags, inplace=True)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Tnere are only a few missing here, but they are more clock events, so we'll drop them\n",
    "-------------------------------------\n",
    "drive_end_transition\n",
    "drive_game_clock_start\n",
    "drive_start_transition\n",
    "drive_game_clock_end\n",
    "drive_time_of_possession\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "data_df.drop(columns=[\n",
    "    'drive_end_transition',\n",
    "    'drive_game_clock_start',\n",
    "    'drive_start_transition',\n",
    "    'drive_game_clock_end',\n",
    "    'drive_time_of_possession'\n",
    "], inplace=True)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Wind up"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 14 create new metrics with these changes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read in dimensions data:  ../data/raw/dimensions.csv\n"
     ]
    }
   ],
   "source": [
    "dim_control_df = pd.DataFrame()\n",
    "print(\"read in dimensions data: \", DIMENSIONS_DATA)\n",
    "try:\n",
    "    dim_control_df = pd.read_csv(DIMENSIONS_DATA)\n",
    "except Exception:\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "outputs": [],
   "source": [
    "metrics = GetMetrics()\n",
    "\n",
    "fact_metrics_df = metrics.get_metrics(data_df, dim_control_df)\n",
    "fact_categories_df = metrics.get_categories(data_df=data_df, unique_count_threshold=40)\n",
    "\n",
    "# db = DatabaseLoader(relative_dir=\"../working_data/fact_metrics.db\")\n",
    "db = DatabaseLoader(connection_string_env_url=\"DB_CONNECTION_URL\")\n",
    "\n",
    "db.load_table(fact_metrics_df, METRICS_OUTPUT_TABLE_NAME)\n",
    "db.load_table(fact_categories_df, CATEGORY_OUTPUT_TABLE_NAME)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No nulls found - Yahoo\n"
     ]
    }
   ],
   "source": [
    "from src.features.wrangling.get_metrics import plot_missing\n",
    "\n",
    "if data_df.isna().sum().sum() == 0:\n",
    "    print(\"No nulls found - Yahoo\")\n",
    "else:\n",
    "    print(\"Showing only the columns that have at least some missing data\")\n",
    "    plot_missing(fact_metrics_df, missing_only=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 15 save data to disk\n",
    "can only really save these interim files for small data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "outputs": [],
   "source": [
    "data_df.to_parquet(GAMEPLAY_FACTS_DF_NAME, engine='fastparquet', compression='snappy')\n",
    "dimensions_df.to_parquet(GAMEPLAY_DIM_DF_NAME, engine='fastparquet', compression='snappy')\n",
    "analytics_df.to_parquet(ANALYTICS_DF_NAME, engine='fastparquet', compression='snappy')\n",
    "admin_df.to_parquet(ADMIN_DF_NAME, engine='fastparquet', compression='snappy')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "outputs": [],
   "source": [
    "with open(READ_ME, 'w') as f:\n",
    "    f.write(f\"\\n{os.path.basename(GAMEPLAY_FACTS_DF_NAME)}\\ta clean version of gameplay with just the core facts\")\n",
    "    f.write(f\"\\n{os.path.basename(GAMEPLAY_DIM_DF_NAME)}\\ta less-clean version of gameplay dimensions that are only non-null for specific kinds of facts\")\n",
    "    f.write(f\"\\n{os.path.basename(ANALYTICS_DF_NAME)}\\tmove all probabilities and stats into a separate dataset - they could be useful later\")\n",
    "    f.write(f\"\\n{os.path.basename(ADMIN_DF_NAME)}\\tmove all gameplay records that are not really plays (e.g. 'Quarter end' to this dataset\")\n",
    "    f.write(f\"\\n{os.path.basename(METRICS_OUTPUT_TABLE_NAME)}.csv\\toptionally - we might save the metrics in a file instead of a database\")\n",
    "    f.write(f\"\\n{os.path.basename(CATEGORY_OUTPUT_TABLE_NAME)}.csv\\toptionally we might save the metrics categories in a file instead of a database\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
