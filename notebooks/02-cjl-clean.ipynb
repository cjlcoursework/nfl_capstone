{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Summary\n",
    "\n",
    "## inputs\n",
    "This notebook has three inputs:\n",
    "1. A reviewed and slightly cleaned version of NFL **nflplaybyplay2009to2016** having many rows per game - each row is a 'play' in the games\n",
    "2. A **dimensions** dataset from our initial review notebook - this categorized each column by how it should be treated\n",
    "3. An NFL **nfl_teams_scraped** dataset that matches team names to the abbreviation used in the gameplay data  (e.g. Green Bay Packers == GB)\n",
    "    3.1 I've since found another list on kaggle - but this one works well enough\n",
    "\n",
    "The gameplay data has many nulls(), but they make sense once we recognize that not every field is applicable for every type of play.\n",
    ">> for example,\n",
    "> If a row represents a passing play, then the rushing data does not make sense, so it's all null\n",
    ">\n",
    "\n",
    "\n",
    "## goal\n",
    "To create datasets that might not yet be completely prepared for ML, but can be queried for many uses, including ML\n",
    "\n",
    "## cleanup\n",
    "1. Separate the data into core **facts** - these are columns that apply to every play, and should never be null\n",
    "2. Create a separate dataset for all the **dimensions** columns that are only good for specific kinds of plays\n",
    "3. Add in facts that are inferred by the sparse dimensions columns, but don't explicitly exist as facts:\n",
    ">> for example:\n",
    ">     If there was a defensive two point conversion - the def_two_point will be non-null\n",
    ">           but it is null for every other case\n",
    ">           see we create a def_two_point_key that is always 1 or 0 in the fact table\n",
    ">           and we move def_two_point the dimensions\n",
    "> There are cases where we could just fill the def_two_point with 'Not Applicable' when it's null,\n",
    "> but that's not ging to solve every issue\n",
    ">\n",
    "4. Identify boolean keys that are important pivots in the facts table:\n",
    "    (a) whether a pass was attempted\n",
    "    (b) whether a RUSH was attempted\n",
    "    (c) whether there was a penalty on the play\n",
    "    (d) an offensive or defensive two point conversion\n",
    "    (e) whether there was a sack\n",
    "    (f) whether a pass was attempted\n",
    " ... and more...\n",
    "\n",
    "## outputs\n",
    "1. A cleaned NFL `gameplay` dataset - having many rows per game - each row is a 'play' in the games\n",
    "2. A column-level metrics dataset that holds some key metrics from describe(), dtypes, etc. and also some configurations\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## metrics\n",
    "Looking at the metrics data - the final output gameplay dataset should be almost complete, with a small enough set of nulls that can be reviewd manually (52)\n",
    "The completeness column is just the amount of non-null records divided by the total record count"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<span><img src=\"metrics_clean_01.png\" width=\"2500\"></span>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TODO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# todo - is the play_recorded key really helpful?\n",
    "# todo - remove the inconsistent playtype column or update it\n",
    "# todo - review the No Play conversion - if the playtype is no good, why fix it halfway?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import re\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "# comments: <span style=\"color:#20B2AA\">\n",
    "\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding modules /Users/christopherlomeli/Source/courses/datascience/nfl_capstone/src\n"
     ]
    }
   ],
   "source": [
    "module_path = os.path.abspath(os.path.join('../src'))\n",
    "print(\"Adding modules\", module_path)\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from src.features.wrangling.database_loader import DatabaseLoader\n",
    "from src.features.wrangling.get_metrics import GetMetrics, update_by_lookp\n",
    "from src.data.s3utils import download_from_s3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "\n",
    "RAW_DATA_PATH = '../data/raw'\n",
    "INTERIM_DATA_PATH='../data/interim'\n",
    "USE_CONNECTION=\"DB_FILENAME_URL\"   # DB_FILENAME_URL for csv or DB_CONNECTION_URL for postgres\n",
    "\n",
    "#inputs\n",
    "DATA_FILE = os.path.join(INTERIM_DATA_PATH,\"nflplaybyplay2009to2016_reviewed_01.parquet\")\n",
    "TEAMS_DATA = os.path.join(RAW_DATA_PATH,\"nfl_teams_scraped.csv\")\n",
    "DIMENSIONS_DATA = os.path.join(RAW_DATA_PATH,\"dimensions.csv\")\n",
    "\n",
    "#outputs\n",
    "GAMEPLAY_FACTS_DF_NAME=os.path.join(INTERIM_DATA_PATH, \"gameplay_facts_cleaned_01.parquet\")\n",
    "GAMEPLAY_DIM_DF_NAME=os.path.join(INTERIM_DATA_PATH, \"gameplay_dimensions_cleaned_01.parquet\")\n",
    "ANALYTICS_DF_NAME=os.path.join(INTERIM_DATA_PATH, \"analytic_events_cleaned_01.parquet\")\n",
    "ADMIN_DF_NAME=os.path.join(INTERIM_DATA_PATH, \"admin_events_cleaned_01.parquet\")\n",
    "READ_ME = os.path.join(INTERIM_DATA_PATH,\"README.02-cjl-clean.txt\")\n",
    "\n",
    "# tables\n",
    "METRICS_INPUT_TABLE_NAME=\"nfl_metrics\"\n",
    "CATEGORY_OUTPUT_TABLE_NAME=\"nfl_cleaned_categories\"\n",
    "METRICS_OUTPUT_TABLE_NAME=\"nfl_cleaned_metrics\"\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## read data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already exists:  /Users/christopherlomeli/Source/courses/datascience/nfl_capstone/data/raw/dimensions.csv\n",
      "we don't need games.csv right now.\n",
      "we don't need nfl_stadiums.csv right now.\n",
      "we don't need nfl_teams.csv right now.\n",
      "already exists:  /Users/christopherlomeli/Source/courses/datascience/nfl_capstone/data/raw/nfl_teams_scraped.csv\n",
      "we don't need NFL Play by Play 2009-2016 (v3).csv right now.\n",
      "we don't need NFL Play by Play 2009-2017 (v4).csv right now.\n",
      "we don't need NFL Play by Play 2009-2018 (v5).csv right now.\n",
      "we don't need spreadspoke_scores.csv right now.\n"
     ]
    }
   ],
   "source": [
    "download_from_s3(prefix=\"nfl_capstone/data/raw\", local_dir=os.path.abspath(RAW_DATA_PATH), wishlist=['nfl_teams_scraped.csv', 'dimensions.csv'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "if not os.path.exists(DATA_FILE):\n",
    "    raise Exception(f\"Can't find the input file {DATA_FILE} .  Have you run the preceeding notebooks? \")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "         date     game_id  drive  qtr    down   time  time_under  time_secs  \\\n0  2009-09-10  2009091000      1    1     NaN  15:00          15 3600.00000   \n1  2009-09-10  2009091000      1    1 1.00000  14:53          15 3593.00000   \n2  2009-09-10  2009091000      1    1 2.00000  14:16          15 3556.00000   \n3  2009-09-10  2009091000      1    1 3.00000  13:35          14 3515.00000   \n4  2009-09-10  2009091000      1    1 4.00000  13:27          14 3507.00000   \n\n   play_time_diff sideof_field  ...  yac_epa  home_wp_pre  away_wp_pre  \\\n0         0.00000          TEN  ...      NaN      0.48567      0.51433   \n1         7.00000          PIT  ...  1.14608      0.54643      0.45357   \n2        37.00000          PIT  ...      NaN      0.55109      0.44891   \n3        41.00000          PIT  ... -5.03142      0.51079      0.48921   \n4         8.00000          PIT  ...      NaN      0.46122      0.53878   \n\n   home_wp_post  away_wp_post  win_prob      wpa  air_wpa  yac_wpa  season  \n0       0.54643       0.45357   0.48567  0.06076      NaN      NaN    2009  \n1       0.55109       0.44891   0.54643  0.00465 -0.03224  0.03690    2009  \n2       0.51079       0.48921   0.55109 -0.04029      NaN      NaN    2009  \n3       0.46122       0.53878   0.51079 -0.04958  0.10666 -0.15624    2009  \n4       0.55893       0.44107   0.46122  0.09771      NaN      NaN    2009  \n\n[5 rows x 102 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>game_id</th>\n      <th>drive</th>\n      <th>qtr</th>\n      <th>down</th>\n      <th>time</th>\n      <th>time_under</th>\n      <th>time_secs</th>\n      <th>play_time_diff</th>\n      <th>sideof_field</th>\n      <th>...</th>\n      <th>yac_epa</th>\n      <th>home_wp_pre</th>\n      <th>away_wp_pre</th>\n      <th>home_wp_post</th>\n      <th>away_wp_post</th>\n      <th>win_prob</th>\n      <th>wpa</th>\n      <th>air_wpa</th>\n      <th>yac_wpa</th>\n      <th>season</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2009-09-10</td>\n      <td>2009091000</td>\n      <td>1</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>15:00</td>\n      <td>15</td>\n      <td>3600.00000</td>\n      <td>0.00000</td>\n      <td>TEN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>0.48567</td>\n      <td>0.51433</td>\n      <td>0.54643</td>\n      <td>0.45357</td>\n      <td>0.48567</td>\n      <td>0.06076</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2009</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2009-09-10</td>\n      <td>2009091000</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1.00000</td>\n      <td>14:53</td>\n      <td>15</td>\n      <td>3593.00000</td>\n      <td>7.00000</td>\n      <td>PIT</td>\n      <td>...</td>\n      <td>1.14608</td>\n      <td>0.54643</td>\n      <td>0.45357</td>\n      <td>0.55109</td>\n      <td>0.44891</td>\n      <td>0.54643</td>\n      <td>0.00465</td>\n      <td>-0.03224</td>\n      <td>0.03690</td>\n      <td>2009</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2009-09-10</td>\n      <td>2009091000</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2.00000</td>\n      <td>14:16</td>\n      <td>15</td>\n      <td>3556.00000</td>\n      <td>37.00000</td>\n      <td>PIT</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>0.55109</td>\n      <td>0.44891</td>\n      <td>0.51079</td>\n      <td>0.48921</td>\n      <td>0.55109</td>\n      <td>-0.04029</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2009</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2009-09-10</td>\n      <td>2009091000</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3.00000</td>\n      <td>13:35</td>\n      <td>14</td>\n      <td>3515.00000</td>\n      <td>41.00000</td>\n      <td>PIT</td>\n      <td>...</td>\n      <td>-5.03142</td>\n      <td>0.51079</td>\n      <td>0.48921</td>\n      <td>0.46122</td>\n      <td>0.53878</td>\n      <td>0.51079</td>\n      <td>-0.04958</td>\n      <td>0.10666</td>\n      <td>-0.15624</td>\n      <td>2009</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2009-09-10</td>\n      <td>2009091000</td>\n      <td>1</td>\n      <td>1</td>\n      <td>4.00000</td>\n      <td>13:27</td>\n      <td>14</td>\n      <td>3507.00000</td>\n      <td>8.00000</td>\n      <td>PIT</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>0.46122</td>\n      <td>0.53878</td>\n      <td>0.55893</td>\n      <td>0.44107</td>\n      <td>0.46122</td>\n      <td>0.09771</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2009</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 102 columns</p>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data file\n",
    "data_df = pd.read_parquet(DATA_FILE)\n",
    "data_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "db = DatabaseLoader(connection_string_env_url=USE_CONNECTION)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "          column_name data_type  unique_counts feature_type     c_dimension  \\\n0       def_two_point    object              2     category  dim(twoopoint)   \n1     blocking_player    object            101     category      dim(block)   \n2      two_point_conv    object              2     category             NaN   \n3  chal_replay_result    object              2          key            fact   \n4     rec_fumb_player    object           1827     category     dim(fumble)   \n\n   row_count  good_count  missing_count  completeness quality  mean  std  min  \\\n0     407688          24         407664       0.00006    poor   NaN  NaN  NaN   \n1     407688         117         407571       0.00029    poor   NaN  NaN  NaN   \n2     407688         605         407083       0.00148    poor   NaN  NaN  NaN   \n3     407688        3402         404286       0.00834    poor   NaN  NaN  NaN   \n4     407688        4373         403315       0.01073    poor   NaN  NaN  NaN   \n\n   max  median       top       freq  \n0  NaN     NaN   Failure   19.00000  \n1  NaN     NaN  D.Watson    3.00000  \n2  NaN     NaN   Failure  322.00000  \n3  NaN     NaN    Upheld 1986.00000  \n4  NaN     NaN   M.Adams   15.00000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>column_name</th>\n      <th>data_type</th>\n      <th>unique_counts</th>\n      <th>feature_type</th>\n      <th>c_dimension</th>\n      <th>row_count</th>\n      <th>good_count</th>\n      <th>missing_count</th>\n      <th>completeness</th>\n      <th>quality</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>min</th>\n      <th>max</th>\n      <th>median</th>\n      <th>top</th>\n      <th>freq</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>def_two_point</td>\n      <td>object</td>\n      <td>2</td>\n      <td>category</td>\n      <td>dim(twoopoint)</td>\n      <td>407688</td>\n      <td>24</td>\n      <td>407664</td>\n      <td>0.00006</td>\n      <td>poor</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Failure</td>\n      <td>19.00000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>blocking_player</td>\n      <td>object</td>\n      <td>101</td>\n      <td>category</td>\n      <td>dim(block)</td>\n      <td>407688</td>\n      <td>117</td>\n      <td>407571</td>\n      <td>0.00029</td>\n      <td>poor</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>D.Watson</td>\n      <td>3.00000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>two_point_conv</td>\n      <td>object</td>\n      <td>2</td>\n      <td>category</td>\n      <td>NaN</td>\n      <td>407688</td>\n      <td>605</td>\n      <td>407083</td>\n      <td>0.00148</td>\n      <td>poor</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Failure</td>\n      <td>322.00000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>chal_replay_result</td>\n      <td>object</td>\n      <td>2</td>\n      <td>key</td>\n      <td>fact</td>\n      <td>407688</td>\n      <td>3402</td>\n      <td>404286</td>\n      <td>0.00834</td>\n      <td>poor</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Upheld</td>\n      <td>1986.00000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>rec_fumb_player</td>\n      <td>object</td>\n      <td>1827</td>\n      <td>category</td>\n      <td>dim(fumble)</td>\n      <td>407688</td>\n      <td>4373</td>\n      <td>403315</td>\n      <td>0.01073</td>\n      <td>poor</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>M.Adams</td>\n      <td>15.00000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the metrics file we used to inspect the data manually\n",
    "metrics_df = db.read_table(METRICS_INPUT_TABLE_NAME)\n",
    "metrics_df.head()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conversions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 01 rename key indicators with a _key postfix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "          column_name data_type  unique_counts feature_type     c_dimension  \\\n0       def_two_point    object              2     category  dim(twoopoint)   \n1     blocking_player    object            101     category      dim(block)   \n2      two_point_conv    object              2     category             NaN   \n3  chal_replay_result    object              2          key            fact   \n4     rec_fumb_player    object           1827     category     dim(fumble)   \n\n   row_count  good_count  missing_count  completeness quality  mean  std  min  \\\n0     407688          24         407664       0.00006    poor   NaN  NaN  NaN   \n1     407688         117         407571       0.00029    poor   NaN  NaN  NaN   \n2     407688         605         407083       0.00148    poor   NaN  NaN  NaN   \n3     407688        3402         404286       0.00834    poor   NaN  NaN  NaN   \n4     407688        4373         403315       0.01073    poor   NaN  NaN  NaN   \n\n   max  median       top       freq  \n0  NaN     NaN   Failure   19.00000  \n1  NaN     NaN  D.Watson    3.00000  \n2  NaN     NaN   Failure  322.00000  \n3  NaN     NaN    Upheld 1986.00000  \n4  NaN     NaN   M.Adams   15.00000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>column_name</th>\n      <th>data_type</th>\n      <th>unique_counts</th>\n      <th>feature_type</th>\n      <th>c_dimension</th>\n      <th>row_count</th>\n      <th>good_count</th>\n      <th>missing_count</th>\n      <th>completeness</th>\n      <th>quality</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>min</th>\n      <th>max</th>\n      <th>median</th>\n      <th>top</th>\n      <th>freq</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>def_two_point</td>\n      <td>object</td>\n      <td>2</td>\n      <td>category</td>\n      <td>dim(twoopoint)</td>\n      <td>407688</td>\n      <td>24</td>\n      <td>407664</td>\n      <td>0.00006</td>\n      <td>poor</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Failure</td>\n      <td>19.00000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>blocking_player</td>\n      <td>object</td>\n      <td>101</td>\n      <td>category</td>\n      <td>dim(block)</td>\n      <td>407688</td>\n      <td>117</td>\n      <td>407571</td>\n      <td>0.00029</td>\n      <td>poor</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>D.Watson</td>\n      <td>3.00000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>two_point_conv</td>\n      <td>object</td>\n      <td>2</td>\n      <td>category</td>\n      <td>NaN</td>\n      <td>407688</td>\n      <td>605</td>\n      <td>407083</td>\n      <td>0.00148</td>\n      <td>poor</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Failure</td>\n      <td>322.00000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>chal_replay_result</td>\n      <td>object</td>\n      <td>2</td>\n      <td>key</td>\n      <td>fact</td>\n      <td>407688</td>\n      <td>3402</td>\n      <td>404286</td>\n      <td>0.00834</td>\n      <td>poor</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Upheld</td>\n      <td>1986.00000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>rec_fumb_player</td>\n      <td>object</td>\n      <td>1827</td>\n      <td>category</td>\n      <td>dim(fumble)</td>\n      <td>407688</td>\n      <td>4373</td>\n      <td>403315</td>\n      <td>0.01073</td>\n      <td>poor</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>M.Adams</td>\n      <td>15.00000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed chal_replay_result to chal_replay_result_key\n",
      "Renamed timeout_indicator to timeout_indicator_key\n",
      "Renamed play_attempted to play_attempted_key\n",
      "Renamed sp to sp_key\n",
      "Renamed touchdown to touchdown_key\n",
      "Renamed safety to safety_key\n",
      "Renamed onsidekick to onsidekick_key\n",
      "Renamed pass_attempt to pass_attempt_key\n",
      "Renamed qb_hit to qb_hit_key\n",
      "Renamed interception_thrown to interception_thrown_key\n",
      "Renamed rush_attempt to rush_attempt_key\n",
      "Renamed reception to reception_key\n",
      "Renamed fumble to fumble_key\n",
      "Renamed sack to sack_key\n"
     ]
    }
   ],
   "source": [
    "for col in metrics_df.loc[(metrics_df.feature_type=='key'), 'column_name']:\n",
    "     new_name = f\"{col}_key\".replace(\".\",\"_\")\n",
    "     if col in data_df.columns:\n",
    "        data_df.rename(columns={col: new_name}, inplace=True)\n",
    "        print(f\"Renamed {col} to {new_name}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHANGED  chal_replay_result_key\n",
      "CHANGED  timeout_indicator_key\n",
      "CHANGED  play_attempted_key\n",
      "CHANGED  sp_key\n",
      "CHANGED  touchdown_key\n",
      "CHANGED  safety_key\n",
      "CHANGED  onsidekick_key\n",
      "CHANGED  pass_attempt_key\n",
      "CHANGED  qb_hit_key\n",
      "CHANGED  interception_thrown_key\n",
      "CHANGED  rush_attempt_key\n",
      "CHANGED  reception_key\n",
      "CHANGED  fumble_key\n",
      "CHANGED  sack_key\n"
     ]
    }
   ],
   "source": [
    "for col in metrics_df.loc[(metrics_df.feature_type=='key'), 'column_name']:\n",
    "    new_name = f\"{col}_key\".replace(\".\",\"_\")\n",
    "    if new_name in data_df.columns:\n",
    "        print(\"CHANGED \", new_name)\n",
    "    else:\n",
    "        print(\"BAD - did not change \", col)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 02 add additional fact keys to the database based on query"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# create a little function to add a new key based on another clumn\n",
    "def add_key(newkey, depends_on):\n",
    "    data_df[newkey] = 0\n",
    "    data_df.loc[(data_df[depends_on].notnull()), newkey] = 1\n",
    "\n",
    "# add keys base on nullity of another field\n",
    "add_key(\"def_two_point_key\", \"def_two_point\" )\n",
    "add_key(\"ex_point_result_key\", \"ex_point_result\" )\n",
    "add_key(\"return_key\", \"returner\" )\n",
    "add_key(\"tackle_key\", \"tackler1\" )\n",
    "add_key(\"two_point_conv_key\", \"two_point_conv\" )\n",
    "\n",
    "# add penalty_key if penalty.yards > 0\n",
    "data_df[\"penalty_key\"] = 0\n",
    "data_df.loc[(data_df[\"penalty_yards\"] > 0), \"penalty_key\"] = 1\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 03 move analytics to a separate dataframe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# save analytics columns to a separate df, and remove from the dataset\n",
    "all_columns = set(data_df.columns)\n",
    "analytics_columns = set(metrics_df.loc[(metrics_df.feature_type == 'analytics'), 'column_name'])\n",
    "\n",
    "analytics_df =  data_df[analytics_columns].copy()\n",
    "data_df.drop(columns=analytics_columns, inplace=True)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 04 update the playattempted column\n",
    "Several rows are for administrative events such as timeout, END Game, or End Quarter -- which creates a lot of nulls and not-applicable values\n",
    "We probably don't want them, so at least segment them as gameplay = Yes or No, where gameplay=No signifies an administrative event, not a real play"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "data_df[\"playattempted\"] = 1\n",
    "\n",
    "# maybe drop or add category [gameplay vs admin]\n",
    "data_df.loc[data_df[\"play_type\"].isin([\n",
    "    'Quarter End',\n",
    "    'Two Minute Warning',\n",
    "    'End of Game',\n",
    "    'Half End',\n",
    "    'Timeout'\n",
    "]), \"play_attempted\"] = 0\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review results - admin events should now have a gameplay == 0\n"
     ]
    },
    {
     "data": {
      "text/plain": "Series([], dtype: int64)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Review results - admin events should now have a gameplay == 0\")\n",
    "data_df.loc[data_df.playattempted == 0, ['play_attempted', 'play_type']].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 05 fill in missing passers\n",
    "In some cases the passer is NaN - in these cases we can get it from the passer_id from other good records"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# create a unique list of passer_id and passer name\n",
    "passers_df = data_df.loc[(data_df.passer.notna()) & (data_df.passer_id.notna()) & (data_df.passer_id!='None') , ['passer', 'passer_id']]\n",
    "\n",
    "# use this utility to update\n",
    "data_df = update_by_lookp(\n",
    "    left_df=data_df, left_col='passer', left_on='passer_id',\n",
    "    lookup_df=passers_df, lookup_col='passer_fix', lookup_on='passer_id', nulls_only=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 06 add playrecorded_key column\n",
    "There are many `playtype`==\"No Play\" rows, and they are only \"No Play\" because the play was replayed due to penalty or some other event.\n",
    "But the initial play had a value, such as Pass, Punt, etc.\n",
    "We want to know what the original play was, so create a different column for No play where if `play_recorded`=False then it's a reply\n",
    "We then update to the original `playtype` in the `playtype` field and use the `play_recorded` field to know whether there was a replay (play_recorded=False) or not."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "play_type           play_recorded_key\nPass                1                    159353\nRun                 1                    120831\nKickoff             1                     23403\nPunt                1                     22003\nNo Play             0                     21414\nTimeout             1                     16206\nSack                1                     10649\nExtra Point         1                     10063\nField Goal          1                      8928\nQuarter End         1                      4914\nTwo Minute Warning  1                      3741\nQB Kneel            1                      3530\nEnd of Game         1                      1973\nSpike               1                       640\nHalf End            1                        40\ndtype: int64"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 06 add playrecorded_key column\n",
    "# set play_recorded = True (1) for all rows\n",
    "data_df[\"play_recorded_key\"] = 1\n",
    "\n",
    "# Set the play_recorde to False (0) if the playtype is currently == No Play\n",
    "data_df.loc[data_df[\"play_type\"] == 'No Play', \"play_recorded_key\"] = 0\n",
    "data_df[['play_type', 'play_recorded_key']].value_counts()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 07 convert 'No Play' `playtypes` to their original playtype\n",
    "\"No play\" means that there was some sort of penalty or stoppage, and the play was cancelled - to be replayed.\n",
    "The penalty could have occurred after the play started or before it started (e.g. False Start).\n",
    "If a play actually did get underway then we want to know what the play was.\n",
    "\n",
    "So add a new field (play_recorded) that tells us whether the play was cancelled and needs to be replayed or was counted.\n",
    "And then back-fill the playtype field for these row with the original play (when we can figure it out)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "0         0\n1         1\n2         0\n3         1\n4         0\n         ..\n407683    0\n407684    1\n407685    1\n407686    0\n407687    0\nName: pass_attempt_key, Length: 407688, dtype: int64"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df[\"pass_attempt_key\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining No Plays\n"
     ]
    },
    {
     "data": {
      "text/plain": "play_type   play_recorded_key\nPenalty     0                    9700\nPass        0                    8703\nNo Play     0                    2516\nPunt        0                     366\nField Goal  0                     129\ndtype: int64"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# maybe drop or add category [gameplay vs admin]\n",
    "data_df.loc[(data_df[\"play_type\"] == 'No Play') & (data_df[\"pass_attempt_key\"] == 1), 'play_type'] = 'Pass'\n",
    "data_df.loc[(data_df[\"play_type\"] == 'No Play') & (data_df[\"field_goal_result\"].notna()), 'play_type'] = 'Field Goal'\n",
    "data_df.loc[(data_df[\"play_type\"] == 'No Play') & (data_df[\"punt_result\"].notna()), 'play_type'] = 'Punt'\n",
    "data_df.loc[(data_df[\"play_type\"] == 'No Play') & (data_df[\"penalty_type\"].notna()), 'play_type'] = 'Penalty'\n",
    "\n",
    "print(\"Remaining No Plays\")\n",
    "data_df.loc[(data_df[\"play_recorded_key\"] == 0) , ['play_type', 'play_recorded_key']].value_counts()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 08 populate missing penalties\n",
    "There are many missing `penaltytype` columns, but we can see what they should have been by looking at the desc field.\n",
    "Use that field to parse out the actual `penaltytype` where we can"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# create a function to parse the penalty from the description\"\n",
    "def parse_penalty(value):\n",
    "    result = re.search(r\"(?i)(PENALTY on)([^\\,]*)\\,([^\\,]*)\", value)\n",
    "    try:\n",
    "        v =  result.group(3)\n",
    "    except AttributeError:\n",
    "        v = ''\n",
    "    return v\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the original dataframe (407688, 95)\n",
      "Shape of our updated dataframe (407688, 95)\n",
      "Are there any leftover penalties - this should be empty\n"
     ]
    },
    {
     "data": {
      "text/plain": "Empty DataFrame\nColumns: [desc, penalty_type]\nIndex: []",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>desc</th>\n      <th>penalty_type</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the subset of all rows where we know it's a penalty, but penaltytype is NA\n",
    "missing_penalties = data_df.loc[(data_df['accepted_penalty'] == 1) & (data_df['penalty_type'].isna()) & (data_df['desc'].notna()), ['desc']]\n",
    "\n",
    "# parse the penalty from the description\n",
    "missing_penalties['penalty_fix'] = missing_penalties.desc.apply(parse_penalty)\n",
    "\n",
    "# drop the desc column - we have what we need\n",
    "missing_penalties.drop(columns = ['desc'], inplace=True)\n",
    "\n",
    "# merge our fix in as penalty_fix field\n",
    "df = pd.merge(data_df, missing_penalties, left_index=True, right_index=True, how='outer')\n",
    "df.loc[(data_df['accepted_penalty'] == 1) & (data_df['penalty_type'].isna()) & (df['penalty_fix'].notna()), ['penalty_type', 'penalty_fix']]\n",
    "\n",
    "# replace empty penaltytype with the penalty_fix\n",
    "df.loc[(data_df['accepted_penalty'] == 1) & (data_df['penalty_type'].isna()) & (df['penalty_fix'].notna()), 'penalty_type'] = df['penalty_fix']\n",
    "\n",
    "# drop the fix column\n",
    "df.drop(columns = ['penalty_fix'], inplace=True)\n",
    "print(\"Shape of the original dataframe\", data_df.shape)\n",
    "print(\"Shape of our updated dataframe\", df.shape)\n",
    "\n",
    "# assign df to data_df\n",
    "data_df = df\n",
    "\n",
    "# verify results - should be zero -- assert?\n",
    "print(\"Are there any leftover penalties - this should be empty\")\n",
    "data_df.loc[(data_df['accepted_penalty'] == 1) & (data_df['penalty_type'].isna()) & (data_df['desc'].notna()), ['desc', 'penalty_type']]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 09 validate team names\n",
    "Several fields are populated with the team abbreviation (e.g. LA Rams == 'LAR')\n",
    "Some of these abbreviations are historical and no longer exist\n",
    "Others are errors -"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "['ARI',\n 'ATL',\n 'BAL',\n 'BUF',\n 'CAR',\n 'CHI',\n 'CIN',\n 'CLE',\n 'DAL',\n 'DEN',\n 'DET',\n 'GB',\n 'HOU',\n 'IND',\n 'KC',\n 'MIA',\n 'MIN',\n 'NE',\n 'NO',\n 'NYG',\n 'NYJ',\n 'PHI',\n 'PIT',\n 'SF',\n 'SEA',\n 'TB',\n 'TEN',\n 'WAS',\n 'SD',\n 'LAC',\n 'LV',\n 'OAK',\n 'LAR',\n 'STL',\n 'JAX',\n 'JAC']"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a control list of team names and abbreviations\n",
    "team_df = pd.read_csv(TEAMS_DATA)\n",
    "teams = list(team_df.Abbreviation)\n",
    "teams"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COLUMN:  home_team\n",
      "['LA']\n",
      "-----------------------------------\n",
      "COLUMN:  away_team\n",
      "['LA']\n",
      "-----------------------------------\n",
      "COLUMN:  defensive_team\n",
      "[None 'LA']\n",
      "-----------------------------------\n",
      "COLUMN:  posteam\n",
      "[None 'LA']\n",
      "-----------------------------------\n",
      "COLUMN:  rec_fumb_team\n",
      "[None 'LA']\n",
      "-----------------------------------\n",
      "COLUMN:  timeout_team\n",
      "['None' 'LA']\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# create a function to list al team abbreviations that are not in our control list\n",
    "team_columns = ['home_team', 'away_team', 'defensive_team', 'posteam', 'rec_fumb_team','timeout_team']\n",
    "\n",
    "def validate_teams():\n",
    "    for t in team_columns:\n",
    "        print(\"COLUMN: \", t)\n",
    "        print(data_df.loc[~data_df[t].isin(list(teams)), t].unique())\n",
    "        print(\"-----------------------------------\")\n",
    "\n",
    "validate_teams()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update COLUMN:  home_team\n",
      "Update COLUMN:  away_team\n",
      "Update COLUMN:  defensive_team\n",
      "Update COLUMN:  posteam\n",
      "Update COLUMN:  rec_fumb_team\n",
      "Update COLUMN:  timeout_team\n"
     ]
    }
   ],
   "source": [
    "# cleanup the ones we know about\n",
    "for t in team_columns:\n",
    "    print(\"Update COLUMN: \", t)\n",
    "    data_df.loc[data_df[t]=='LA', t] = 'LAR'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 10 fillna for sparse columns\n",
    "Some categorical columns have null values because the column is not applicable to the play itself.\n",
    "For example, if there was no pass, then the pass-outcome column would be null.\n",
    "So this is really a 'Not Applicable' sort of value\n",
    "We are just cleaning at this point, and we may use this data in many ways, so rather than having a null, it's simple to update tose nulls to 'NA' for these categorical fields.\n",
    "\n",
    "Depending on how we use the data later, we'll drop these, or segment them out for different uses."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chal_replay_result_key Nulls before:  404286 chal_replay_result_key Nulls after:  0\n",
      "----------------------------------------------------------\n",
      "def_two_point Nulls before:  407664 def_two_point Nulls after:  0\n",
      "----------------------------------------------------------\n",
      "ex_point_result Nulls before:  397578 ex_point_result Nulls after:  0\n",
      "----------------------------------------------------------\n",
      "field_goal_result Nulls before:  398629 field_goal_result Nulls after:  0\n",
      "----------------------------------------------------------\n",
      "interceptor Nulls before:  403168 interceptor Nulls after:  0\n",
      "----------------------------------------------------------\n",
      "passer Nulls before:  228359 passer Nulls after:  0\n",
      "----------------------------------------------------------\n",
      "passer_id Nulls before:  0 passer_id Nulls after:  0\n",
      "----------------------------------------------------------\n",
      "pass_length Nulls before:  240520 pass_length Nulls after:  0\n",
      "----------------------------------------------------------\n",
      "pass_location Nulls before:  240520 pass_location Nulls after:  0\n",
      "----------------------------------------------------------\n",
      "pass_outcome Nulls before:  239506 pass_outcome Nulls after:  0\n",
      "----------------------------------------------------------\n",
      "penalized_player Nulls before:  379403 penalized_player Nulls after:  0\n",
      "----------------------------------------------------------\n",
      "penalized_team Nulls before:  378189 penalized_team Nulls after:  0\n",
      "----------------------------------------------------------\n",
      "penalty_yards Nulls before:  0 penalty_yards Nulls after:  0\n",
      "----------------------------------------------------------\n",
      "penalty_type Nulls before:  378189 penalty_type Nulls after:  0\n",
      "----------------------------------------------------------\n",
      "punt_result Nulls before:  385317 punt_result Nulls after:  0\n",
      "----------------------------------------------------------\n",
      "receiver Nulls before:  246127 receiver Nulls after:  0\n",
      "----------------------------------------------------------\n",
      "rec_fumb_player Nulls before:  403315 rec_fumb_player Nulls after:  0\n",
      "----------------------------------------------------------\n",
      "rec_fumb_team Nulls before:  403315 rec_fumb_team Nulls after:  0\n",
      "----------------------------------------------------------\n",
      "returner Nulls before:  382384 returner Nulls after:  0\n",
      "----------------------------------------------------------\n",
      "return_result Nulls before:  389450 return_result Nulls after:  0\n",
      "----------------------------------------------------------\n",
      "run_location Nulls before:  288178 run_location Nulls after:  0\n",
      "----------------------------------------------------------\n",
      "rusher Nulls before:  287124 rusher Nulls after:  0\n",
      "----------------------------------------------------------\n",
      "safety_key Nulls before:  0 safety_key Nulls after:  0\n",
      "----------------------------------------------------------\n",
      "two_point_conv Nulls before:  407083 two_point_conv Nulls after:  0\n",
      "----------------------------------------------------------\n",
      "run_gap Nulls before:  320260 run_gap Nulls after:  0\n",
      "----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "sparse_columns = ['chal_replay_result_key',\n",
    "                  'def_two_point',\n",
    "                  'ex_point_result',\n",
    "                  'field_goal_result',\n",
    "                  'interceptor',\n",
    "                  'passer',\n",
    "                  'passer_id',\n",
    "                  'pass_length',\n",
    "                  'pass_location',\n",
    "                  'pass_outcome',\n",
    "                  'penalized_player',\n",
    "                  'penalized_team',\n",
    "                  'penalty_yards',\n",
    "                  'penalty_type',\n",
    "                  'punt_result',\n",
    "                  'receiver',\n",
    "                  'rec_fumb_player',\n",
    "                  'rec_fumb_team',\n",
    "                  'returner',\n",
    "                  'return_result',\n",
    "                  'run_location',\n",
    "                  'rusher',\n",
    "                  'safety_key',\n",
    "                  'two_point_conv',\n",
    "                  'run_gap']\n",
    "\n",
    "for col in sparse_columns:\n",
    "    print(f\"{col} Nulls before:  {data_df[col].isna().sum()}\", end=\" \")\n",
    "    data_df[col].fillna(\"NA\", inplace = True)\n",
    "    print(f\"{col} Nulls after:  {data_df[col].isna().sum()}\")\n",
    "    print('----------------------------------------------------------')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Segment into Gameplay, Admin, Analytics datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 11 separate gameplay events from admin events to separate dataframes\n",
    "These tend to have a lot of nulls because they are really not gameplay events - and they might not be good for all kinds of predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original shape (407688, 95)\n",
      "admin_events shape (0, 95)\n",
      "game_events shape (407688, 95)\n"
     ]
    }
   ],
   "source": [
    "print(\"original shape\", data_df.shape)\n",
    "\n",
    "admin_events_df = data_df.loc[data_df.playattempted != 1].copy()\n",
    "print(\"admin_events shape\", admin_events_df.shape)\n",
    "\n",
    "gameplay_facts_df = data_df.loc[data_df.playattempted == 1].copy()\n",
    "print(\"game_events shape\", gameplay_facts_df.shape)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Facts :: {'goal_to_go', 'yards_gained', 'pos_team_score', 'first_down', 'away_team', 'play_attempted', 'timeout_indicator', 'sp', 'rush_attempt', 'sack', 'down', 'yrdline100', 'abs_score_diff', 'posteam_timeouts_pre', 'play_time_diff', 'home_team', 'game_id', 'onsidekick', 'ydsnet', 'ydstogo', 'chal_replay_result', 'def_team_score', 'date', 'posteam', 'desc', 'qtr', 'pass_attempt', 'reception', 'defensive_team', 'yards_after_catch', 'safety', 'drive', 'yrdln', 'qb_hit', 'time_secs', 'time_under', 'fumble', 'play_type', 'sideof_field', 'score_diff', 'time', 'season', 'interception_thrown', 'touchdown'}\n",
      "Sparse dimensions :: {'ex_point_prob', 'ex_point_result', 'tackler2', 'home_timeouts_remaining_post', 'returner', 'receiver_id', 'rusher_id', 'rec_fumb_player', 'tackler1', 'penalized_team', 'accepted_penalty', 'run_location', 'penalty_type', 'two_point_prob', 'pass_outcome', 'field_goal_prob', 'interceptor', 'safety_prob', 'air_yards', 'blocking_player', 'timeout_team', 'penalized_player', 'home_timeouts_remaining_pre', 'challenge_replay', 'rusher', 'passer', 'field_goal_distance', 'def_two_point', 'rec_fumb_team', 'two_point_conv', 'pass_location', 'punt_result', 'exp_pts', 'return_result', 'receiver', 'away_timeouts_remaining_pre', 'penalty_yards', 'pass_length', 'passer_id', 'run_gap', 'field_goal_result', 'away_timeouts_remaining_post'}\n"
     ]
    }
   ],
   "source": [
    "gameplay_facts_columns = set(metrics_df.loc[(metrics_df.c_dimension == 'fact'), 'column_name'])\n",
    "gameplay_dim_columns = set(metrics_df.loc[(metrics_df.c_dimension != 'fact'), 'column_name'])\n",
    "\n",
    "all_gameplay_columns = set(gameplay_facts_df.columns)\n",
    "\n",
    "adjusted_dim_columns = gameplay_dim_columns.intersection(all_gameplay_columns)  # we already got rid of the analytics columns - so make sure we are only choosing columns that still exists\n",
    "\n",
    "print(\"Facts ::\", gameplay_facts_columns)\n",
    "print(\"Sparse dimensions ::\", adjusted_dim_columns)\n",
    "\n",
    "gameplay_dimensions_df = gameplay_facts_df[adjusted_dim_columns].copy()\n",
    "gameplay_facts_df.drop(columns=adjusted_dim_columns, inplace=True)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 12 fillna down and firstdown to zero"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "Series([], Name: play_type, dtype: int64)"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gameplay_facts_df.down = gameplay_facts_df.down.fillna(0)\n",
    "gameplay_facts_df.first_down = gameplay_facts_df.first_down.fillna(0)\n",
    "gameplay_facts_df.loc[(gameplay_facts_df.down.isna()), 'play_type'].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 13 set firstdown to zero for Kickoffs and Extra points"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "Series([], Name: play_type, dtype: int64)"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# firstdown\n",
    "\n",
    "gameplay_facts_df.loc[(gameplay_facts_df.first_down.isna()) & (gameplay_facts_df.play_type.isin(['Kickoff', 'Extra Point'])), 'first_down'] = 0\n",
    "gameplay_facts_df.loc[(gameplay_facts_df.first_down.isna()), 'play_type'].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Wind up"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 14 create new metrics with these changes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read in dimensions data:  ../data/raw/dimensions.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": "         column_name feature_type     c_dimension\n0     abs_score_diff   continuous            fact\n1  accepted._penalty     category   dim (penalty)\n2            air_epa    analytics       analytics\n3            air_wpa    analytics       analytics\n4          air_yards   continuous  dim(pass, run)",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>column_name</th>\n      <th>feature_type</th>\n      <th>c_dimension</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>abs_score_diff</td>\n      <td>continuous</td>\n      <td>fact</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>accepted._penalty</td>\n      <td>category</td>\n      <td>dim (penalty)</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>air_epa</td>\n      <td>analytics</td>\n      <td>analytics</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>air_wpa</td>\n      <td>analytics</td>\n      <td>analytics</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>air_yards</td>\n      <td>continuous</td>\n      <td>dim(pass, run)</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimensions_df = pd.DataFrame()\n",
    "print(\"read in dimensions data: \", DIMENSIONS_DATA)\n",
    "try:\n",
    "    dimensions_df = pd.read_csv(DIMENSIONS_DATA)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "dimensions_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "metrics = GetMetrics()\n",
    "\n",
    "fact_metrics_df = metrics.get_metrics(gameplay_facts_df, dimensions_df)\n",
    "fact_categories_df = metrics.get_categories(data_df=gameplay_facts_df, unique_count_threshold=40)\n",
    "\n",
    "# db = DatabaseLoader(relative_dir=\"../working_data/fact_metrics.db\")\n",
    "db = DatabaseLoader(connection_string_env_url=\"DB_CONNECTION_URL\")\n",
    "\n",
    "db.load_table(fact_metrics_df, METRICS_OUTPUT_TABLE_NAME)\n",
    "db.load_table(fact_categories_df, CATEGORY_OUTPUT_TABLE_NAME)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 15 save data to disk\n",
    "can only really save these interim files for small data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "gameplay_facts_df.to_parquet(GAMEPLAY_FACTS_DF_NAME, engine='fastparquet',  compression='snappy')\n",
    "gameplay_dimensions_df.to_parquet(GAMEPLAY_DIM_DF_NAME, engine='fastparquet',  compression='snappy')\n",
    "analytics_df.to_parquet(ANALYTICS_DF_NAME, engine='fastparquet',  compression='snappy')\n",
    "admin_events_df.to_parquet(ADMIN_DF_NAME, engine='fastparquet',  compression='snappy')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "with open(READ_ME, 'w') as f:\n",
    "    f.write(f\"\\n{os.path.basename(GAMEPLAY_FACTS_DF_NAME)}\\ta clean version of gameplay with just the core facts\")\n",
    "    f.write(f\"\\n{os.path.basename(GAMEPLAY_DIM_DF_NAME)}\\ta less-clean version of gameplay dimensions that are only non-null for specific kinds of facts\")\n",
    "    f.write(f\"\\n{os.path.basename(ANALYTICS_DF_NAME)}\\tmove all probabilities and stats into a separate dataset - they could be useful later\")\n",
    "    f.write(f\"\\n{os.path.basename(ADMIN_DF_NAME)}\\tmove all gameplay records that are not really plays (e.g. 'Quarter end' to this dataset\")\n",
    "    f.write(f\"\\n{os.path.basename(METRICS_OUTPUT_TABLE_NAME)}.csv\\toptionally - we might save the metrics in a file instead of a database\")\n",
    "    f.write(f\"\\n{os.path.basename(CATEGORY_OUTPUT_TABLE_NAME)}.csv\\toptionally we might save the metrics categories in a file instead of a database\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
